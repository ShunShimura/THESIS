\thispagestyle{fancy2}

\section{既存手法と問題点}
画像の時系列データを入力として，オンライン処理で複数の物体を追跡する問題は，複数物体追跡（Multi Object Tracking; MOT）と呼ばれ\cite{luo2021multiple}，防犯カメラに映る歩行者や自動車の追跡などに広く用いられている．しかしながらこれらの手法のほとんどが３次元（空間２次元+時間）データにしか対応していない．特に，本問題のようなスライスとして入力される４次元データをそのまま適用できる手法は未だ開発されていない．
\par
そのためこの章では，まずはじめにMOT問題にしばしば使用されるYOLO+SORT\cite{redmon2016you,alif2024yolov1,bewley2016simple}を既存手法として説明し，その後にそれらを基盤とした提案手法の説明に移る．

\subsection{２次元画像の系列データに対する複数物体追跡}
既存手法の説明の前に，それらの手法が解いている問題設定を先に述べる．
\begin{description}
    \item[処理] 時刻$t$で制御されるオンライン処理．ただし時間間隔$dt$は，カメラなどのフレームレートに依存する．
    \item[入力] 各時刻$t$において，一枚の画像データが与えられる．ただしここでは簡単のため，常に同じ画角で得られた画像を仮定する．
    \item[出力] 時刻$t$で追跡している各物体の状態ベクトル．ここで状態ベクトルとは，位置や速度，大きさなどの情報を含むベクトルのことである．
\end{description}
すなわち，前節で説明した問題設定において，画像データが複数のスライスではなく単一のスライスに変化しただけと考えれば良い．
\par
YOLO+SORTでは，前述の問題を物体検出とReIDという２つの処理に段階的に分けて解く．物体検出とは，ある画像を入力したときに，物体の存在する範囲をバウンディングボックスと呼ばれる矩形領域で検出し，なおかつ犬や猫，自動車といったクラス分類も同時に行うタスクのことをいう．また，画像内に複数の物体が存在していても，すべての各物体に対して同時に矩形領域検出・クラス分類を行う．YOLO（You Only Look Once）は，この物体検出用に開発された深層学習モデルである．またSORT（Simple Online Realtime Tracking）は，YOLOのような物体検出モデルから出力されたバウンディングボックスを入力とし，各個体の軌跡を推定する（図\ref{fig:yolosort}参照）．

\begin{figure}[t]
    \centering    
    \includegraphics[width=.5\textwidth]{fig/nagoya-u.eps}
    \caption{YOLO+SORTの処理}
    \label{fig:yolosort}
\end{figure}

後述の提案手法では，YOLOの処理には変更を加えず，ReIDを４次元データ用に拡張してゆく．そのため，その基盤となるSORTの処理について，詳細を述べる．
\par
まず入力として，YOLOから出力される時刻$t=1,2,...,T$のバウンディングボックス群$O_t = \left\{\bm{o}_t^1, \bm{o}_t^2,...\right\}$が得られていると仮定する．ここで，各バウンディングボックス$\bm{o}_t^i$は，矩形領域を表す中心位置$(x,y)$と幅$w$，高さ$h$の４変数ベクトルである． SORTではこれらの入力に対して以下の処理（図\ref{fig:sort}参照）を時刻$1$から$T-1$まで繰り返し，各個体を表すIDで識別された状態変数配列$S_t = \left[\bm{s}_t^1, \bm{s}_t^2,...\right]$を返す．ここで$\bm{s}_t$は，位置や速度，大きさなどの情報を含んでいる．

\begin{enumerate}
    \item 各個体の状態変数$S_t$から，時刻$t+1$のバウンディングボックス配列の予測$\hat{O}_{t+1}$をカルマンフィルタ\cite{adachi2012kalman}の時間更新によって計算する．ただし$t=1$の場合は$\hat{O}_{t+1} \leftarrow O_t$とする．
    \item 予測されたバウンディングボックス配列$\hat{O}_{t+1}$と与えられた$O_{t+1}$を比較し，いずれの組み合わせが同個体であるかを最適化する．
    \item 各状態変数$\bm{s}_t^i$を，それに対応付けられたバウンディングボックス$\bm{o}_{t+1}^i$を用いて更新して$\bm{s}_{t+1}^i$とし，$S_{t+1}$に加える．
\end{enumerate}

\begin{figure}[t]
    \centering    
    \includegraphics[width=\textwidth]{fig/sort.pdf}
    \caption{SORTのアルゴリズム概要}
    \small
    SORTでは予測，マッチング，更新を時刻$1$から$T-1$まで繰り返す．ただし予測と更新はカルマンフィルタによって行われる．ただしこの図では分かりやすさのために状態変数$\bm{s}_t$を用いずに観測系列$\bm{o}_1,...,\bm{o}_t$で図示している．実際には観測系列を記憶する必要はなく，前時刻の状態変数$\bm{s}_t$のみで予測計算を行う点に注意されたい．
    \label{fig:sort}
\end{figure}

前述の既存手法，YOLO+SORTをそのまま本問題設定に適用することはできない．なぜならば，時刻$t$における入力は画像一枚ではなく，共焦点顕微鏡によって得られた複数の画像であるためだ．もちろん複数のスライスを個別に扱い，単一スライスごとにYOLO+SORTを適用することは可能である．しかしながら物体が鉛直方向に移動する場合には，いずれ対象物体がスライスの高さから外れ，追跡できなくなってしまう．またその場合には，各スライスで得られた各個体の軌跡を，事後的に統合するフレームワークが必要になり，問題が複雑になってしまう．
\par
そこで以下の節では，各時刻で他のスライスを考慮した追跡手法を提案する．

\section{提案手法の概要}
まず初めに，提案手法の設計思想を説明するために，従来の３次元（空間２次元＋時間）複数物体追跡と比較した本問題のような４次元（空間３次元＋時間）複数物体追跡のポイントを述べる．
\begin{enumerate}
    \item 鉛直方向に動く物体を追跡したい．本問題では３次元空間を複数の２次元画像で捉えているため，高さごとにスライスを独立して処理をすると，鉛直方向に動く物体を追従できない．
    \item ３次元空間における状態変数を推定したい．従来の３次元複数物体追跡では，バウンディングボックスに含まれる変数$[x,y,w,h]$及びその速度が目的の状態変数であった．しかしながら，４次元複数物体追跡における目的の状態変数は，３次元的な位置や速度である．すなわち，SORTの内部で働くカルマンフィルタの再設計が必要である．
\end{enumerate}

以上の点を踏まえ，前述のYOLO+SORTの処理に加えて，他のスライスを参照する\textbf{Depth-SORT}と\textbf{SliceKalmanFilter}という処理・計算手法を提案する．提案手法の概要は以下の通りである（図\ref{fig:proposal}参照）．

\begin{enumerate}
    \item 物体検出: YOLOv10により，画像を入力してその画像内の物体をバウンディングボックス$[x,y,w,h]$で検出する．
    \item ReID: SORTと新たに提案するDepth-SORTにより，検出されたバウンディングボックスにIDを付与する．
    \item 状態推定: 新しく設計したカルマンフィルタ，SliceKalmanFilterにより，所望の状態変数$[x,\dot{x},y,\dot{y},z,\dot{z},r]$を推定する．
\end{enumerate}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/proposal.pdf}
    \caption{提案手法の概要}
    \label{fig:proposal}
\end{figure}

以降の節では，各パートについて詳細を述べる．

\section{物体検出: YOLOv10}
YOLO（You Only Look Once）は，物体検出を行う深層学習モデルであり，2016年から2024年までに，version 10（以降versionはvで表記する）まで開発されている．図\ref{fig:yolo_base}に示すように，YOLOは（１）画像の内部表現を獲得するBackbone，（２）バウンディングボックスや信頼度スコアに変換するHead，そして（３）冗長な出力を抑えるNon-Maximum Suppression（以降NMSと呼ぶ）で構成される．このうちBackboneとHeadはニューラルネットワークである．これにより，YOLO以前の手法と比較して格段に処理速度が向上し，YOLOはリアルタイム物体検出手法として確立した．以降，v1からv9に至るまで精度と処理速度の向上が図られてきた．

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/yolo_base.pdf}
    \caption{YOLOの基本的なアーキテクチャ}
    \small
    各図の下部にある$\mathbb{R}^{X}$は，各ステップにおける入力及び出力のテンソルもしくは行列のshapeを表している．ただしここでは初期モデルv1の値を参考にしている．またhead出力後の$30$は，$7 \times 7$のgrid上に区別された領域ごとに，信頼性スコア+バウンディングボックス情報の次元数$5$のベクトルを2つ候補として出力し，さらに$20$クラス分類するための確率を出力するため，$(5 \times 2) + 20 = 30$となっている．ここでは，説明を簡易化するためにNeckを省略した．
    \label{fig:yolo_base}
\end{figure}

しかしながら，唯一ニューラルネットワークを使用しないNon-Maximum Suppressionは，v9に至るまで処理速度を下げてしまう大きな要因であった．そこで2024年に発表されたv10\cite{wang2024yolov10}では，NMSを取り除き，dual-assignmentという機構を用いることでEnd-to-Endなニューラルネットワークとなっている．これにより図\ref{fig:yolov10_performance}に示すように，v10はv9の２倍ほどの高速化に成功している．

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{fig/yolov10_performance.pdf}
    \caption{yoloのバージョン比較（（左）性能と推論速度（右）性能とモデルパラメータ数）}
    \small
    縦軸の\textrm{COCO AP}が性能，横軸の\textrm{Latency}は推論にかかる時間，\textrm{Number of Parameters}はモデルの大きさを表す．
    \label{fig:yolov10_performance}
\end{figure}

前述のとおり本研究ではリアルタイム処理を行うため，少なくとも$n_s$枚の画像の物体検出を時間間隔$dt$以内に終わらせなければならない．そのため，本研究ではもっとも精度-推論時間のバランスが優れているYOLOv10を採用した．

\subsection{YOLOv10のアーキテクチャ}
%%% suspend
%%% 全体のアーキテクチャ，backbone，neck, dual-assignment

\subsection{YOLOv10の学習手順}
YOLOv10の学習では，一般公開されているPre-Trainingされたモデルをダウンロードし，Fine-Tuningを行う．Pre-TrainingとFine-Tuningとは，大規模なデータセットでモデルを学習させた後（Pre-Training），その学習されたパラメータを初期値として小規模なデータセットでさらにモデルを微調整する（Fine-Tuning），学習方法の一つである\cite{radford2018improving}．特に，応用面での学習データセットが少ない場合や，汎化的なモデルを構築したいときに用いられる学習方法である．例えば物体検出であれば，すでに大規模なデータセットが溢れている画像分類で事前学習を行い，その後に小規模な物体検出用のデータセットで学習を行う．これにより，事前学習で画像の良い表現学習が行われるため，物体検出を学習させる際のデータセットは小規模で済む．Fine-Tuningでは図\ref{fig:PTandFT}のように，ニューラルネットワークの最終層のみを学習させるのが一般的である．

\begin{figure}[t]
    \centering    
    \includegraphics[width=\textwidth]{fig/PTandFT.pdf}
    \caption{Pre-TrainingとFine-Tuning}
    \label{fig:PTandFT}
\end{figure}

\section{ReID: SORT+DepthSORT}

\section{状態変数の推定: SliceKalmanFilter}